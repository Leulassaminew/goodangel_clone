{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_2jN-aBz_ZI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EmanuelRiquelme/fast_whisperx\n",
        "%cd fast_whisperx\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ZJ0RoB4DNk",
        "outputId": "a20e6132-d0a7-4d3e-e58f-d36596a1848b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#you will probably need to authenticate to access the repository\n",
        "!git clone https://github.com/goodangel1012/goodangel/tree/main/wav2lip\n",
        "%cd wav2lip\n",
        "!mkdir data\n",
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "IcDkHYjt6JEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to download this [weights](https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fradrabha%5Fm%5Fresearch%5Fiiit%5Fac%5Fin%2FDocuments%2FWav2Lip%5FModels%2Fwav2lip%5Fgan%2Epth&parent=%2Fpersonal%2Fradrabha%5Fm%5Fresearch%5Fiiit%5Fac%5Fin%2FDocuments%2FWav2Lip%5FModels&ga=1) and move them to wav2lip/checkpoints"
      ],
      "metadata": {
        "id": "jPv3v1EVqjV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import torch\n",
        "import time\n",
        "import requests\n",
        "import openai\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "openai.api_key  = ''\n",
        "eleven_labels_api_key = ''"
      ],
      "metadata": {
        "id": "rgLL2gNfe4As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "language = 'en'\n",
        "try:\n",
        "    w_model = whisperx.load_model(\"tiny.en\", device, compute_type='float16',language = language)\n",
        "except:\n",
        "    w_model = whisperx.load_model(\"tiny.en\", device, compute_type='int8',language = language)\n",
        "\n",
        "def generate_transcript(audio):\n",
        "    \"\"\"\n",
        "    the first time it runs takes a few seconds to run, then after that takes faction of seconds.\n",
        "    it could be a good improvement point, when it comes to build the optimizing the code on the the backend of the webapp\n",
        "    \"\"\"\n",
        "    audio = whisperx.load_audio(audio)\n",
        "    try:\n",
        "        return w_model.transcribe(audio, batch_size=512)[\"segments\"][0]['text']\n",
        "    except:\n",
        "        return w_model.transcribe(audio, batch_size=64)[\"segments\"][0]['text']"
      ],
      "metadata": {
        "id": "ZnrZUx9kf-xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mp3_to_wav(name_file):\n",
        "    audio = AudioSegment.from_mp3(f'{name_file}.mp3')\n",
        "    answer = 'data/answer.wav'\n",
        "    audio.export(answer, format=\"wav\")\n",
        "\n",
        "def generate_audio(text,voice_id,model_id ='eleven_monolingual_v1'):\n",
        "    CHUNK_SIZE = 400\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream\"\n",
        "    headers = {\n",
        "        \"Accept\": \"audio/mpeg\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"xi-api-key\": eleven_labels_api_key\n",
        "        }\n",
        "    data = {\n",
        "        \"text\": text,\n",
        "        \"model_id\": model_id,\n",
        "        'optimize_streaming_latency': 4,\n",
        "        \"voice_settings\": {\n",
        "            \"stability\": 0,\n",
        "            \"similarity_boost\": 0\n",
        "            }\n",
        "        }\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "    with open('output_new.mp3', 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
        "                if chunk:\n",
        "                        f.write(chunk)\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        executor.submit(mp3_to_wav, 'output_new')\n",
        "\n",
        "def get_voice_id(voice_name):\n",
        "    url = \"https://api.elevenlabs.io/v1/voices\"\n",
        "    headers = {\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"xi-api-key\": eleven_labels_api_key\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return [voice['voice_id'] for voice in json.loads(response.text)['voices'] if voice['name'] == voice_name][0]\n",
        "\n",
        "def transcript_2_audio_response(transcript,voice_name):\n",
        "    voice_id = get_voice_id(voice_name)\n",
        "    transcript = f'{transcript} in 20 words at max '\n",
        "    messages = [{\"role\": \"user\", \"content\": transcript}]\n",
        "    model = 'gpt-3.5-turbo'\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        )\n",
        "    text = response['choices'][0]['message']['content']\n",
        "    generate_audio(text = text,voice_id = voice_id)\n",
        "\n",
        "\n",
        "def question2answer(audio,voice_name):\n",
        "    \"\"\"\n",
        "    returns a generated voice answering the generated response of the audio question.\n",
        "    audio: is the direction where the audio file containing the question is located.\n",
        "    voice_name: the name of the AI generated voice.\n",
        "    \"\"\"\n",
        "    transcript = generate_transcript(audio)\n",
        "    transcript_2_audio_response(transcript,voice_name)"
      ],
      "metadata": {
        "id": "ovdC7OkGg4Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir, path\n",
        "import numpy as np\n",
        "import scipy, cv2, os, sys, argparse, audio\n",
        "import json, subprocess, random, string\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import torch, face_detection\n",
        "from models import Wav2Lip\n",
        "import platform\n",
        "import time\n",
        "import subprocess\n",
        "import shlex\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DRaaSVg5hMbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_smoothened_boxes(boxes, T):\n",
        "    for i in range(len(boxes)):\n",
        "        if i + T > len(boxes):\n",
        "            window = boxes[len(boxes) - T:]\n",
        "        else:\n",
        "            window = boxes[i : i + T]\n",
        "        boxes[i] = np.mean(window, axis=0)\n",
        "    return boxes\n",
        "\n",
        "def face_detect(images,face_det_batch_size):\n",
        "    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D,\n",
        "    flip_input=False)\n",
        "    batch_size = face_det_batch_size\n",
        "    while 1:\n",
        "        predictions = []\n",
        "        for i in tqdm(range(0, len(images), batch_size)):\n",
        "            predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
        "        break\n",
        "    results = []\n",
        "    pady1, pady2, padx1, padx2 = [0, -1, 0, -1]\n",
        "    for rect, image in zip(predictions, images):\n",
        "        y1 = max(0, rect[1] - pady1)\n",
        "        y2 = min(image.shape[0], rect[3] + pady2)\n",
        "        x1 = max(0, rect[0] - padx1)\n",
        "        x2 = min(image.shape[1], rect[2] + padx2)\n",
        "\n",
        "        results.append([x1, y1, x2, y2])\n",
        "\n",
        "    boxes = np.array(results)\n",
        "    boxes = get_smoothened_boxes(boxes, T=5)\n",
        "    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
        "\n",
        "    del detector\n",
        "    return results\n",
        "\n",
        "def datagen(frames, mels,wav2lip_batch_size,face_det_batch_size):\n",
        "    img_size = 96\n",
        "    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "    face_det_results = face_detect(frames,face_det_batch_size) # BGR2RGB for CNN face detection\n",
        "    for i, m in enumerate(mels):\n",
        "        idx = i%len(frames)\n",
        "        frame_to_save = frames[idx].copy()\n",
        "        face, coords = face_det_results[idx].copy()\n",
        "\n",
        "        face = cv2.resize(face, (img_size, img_size))\n",
        "\n",
        "        img_batch.append(face)\n",
        "        mel_batch.append(m)\n",
        "        frame_batch.append(frame_to_save)\n",
        "        coords_batch.append(coords)\n",
        "\n",
        "        if len(img_batch) >= wav2lip_batch_size:\n",
        "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "            img_masked = img_batch.copy()\n",
        "            img_masked[:, img_size//2:] = 0\n",
        "\n",
        "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
        "            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "    if len(img_batch) > 0:\n",
        "        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "        img_masked = img_batch.copy()\n",
        "        img_masked[:, img_size//2:] = 0\n",
        "\n",
        "        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "        yield img_batch, mel_batch, frame_batch, coords_batch\n",
        "\n",
        "mel_step_size = 16\n",
        "\n",
        "def load_model(checkpoint_path = 'checkpoints/wav2lip_gan.pth'):\n",
        "    model = Wav2Lip()\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    s = checkpoint[\"state_dict\"]\n",
        "    new_s = {}\n",
        "    for k, v in s.items():\n",
        "        new_s[k.replace('module.', '')] = v\n",
        "    model.load_state_dict(new_s)\n",
        "    model = model.to('cuda:0')\n",
        "    return model.eval()"
      ],
      "metadata": {
        "id": "UsYDG8cAhUZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = load_model()\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "t6RYbnqPhaVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(face,audio_source = 'data/answer.wav',model = model,wav2lip_batch_size=1,face_det_batch_size=1,resize_factor=1,stream = False):\n",
        "    \"\"\"\n",
        "    generates the avatar if stream is set to truth the avatar gets streamed and if false gets saved on the folder results.\n",
        "    face: is a video or a picture of the person that will be used for the avatar (if it's a video the face of the person must be shown at all times)\n",
        "    audio_source: is the voice of the avatar, it must be a wav file.\n",
        "    face_det_batch_size = batch size of the face detection algorithm it's recommended to be set at 1, no matter the hardware.\n",
        "    wav2lip_batch_size = batch size of the wav2lip algorithm it's recommended to be set at 1, no matter the hardware.\n",
        "    resize_factor = downsampling factor. 1 equals to keeping the original resolution, increasing the number reduces the quality of the avatar\n",
        "    but improves the speed of the function\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    the first time it runs requires to download some weights so be mindful of that for production\n",
        "    \"\"\"\n",
        "    face_name = face.split('/')[1].split('.')[0]\n",
        "    output_name = f'{face_name}_{720 // resize_factor}p'\n",
        "    if not face.split('.')[-1] in ['jpg', 'png', 'jpeg']:\n",
        "        print('video')\n",
        "        video_stream = cv2.VideoCapture(face)\n",
        "        fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "        full_frames = []\n",
        "        while 1:\n",
        "            still_reading, frame = video_stream.read()\n",
        "            if not still_reading:\n",
        "                video_stream.release()\n",
        "                break\n",
        "            if resize_factor > 1:\n",
        "                frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n",
        "\n",
        "            y1, y2, x1, x2 = [0, -1, 0, -1]\n",
        "            if x2 == -1: x2 = frame.shape[1]\n",
        "            if y2 == -1: y2 = frame.shape[0]\n",
        "\n",
        "            frame = frame[y1:y2, x1:x2]\n",
        "\n",
        "            full_frames.append(frame)\n",
        "    else:\n",
        "        full_frames = [cv2.imread(face)]\n",
        "        fps = 25\n",
        "\n",
        "    wav = audio.load_wav(audio_source, 16000)\n",
        "    mel = audio.melspectrogram(wav)\n",
        "\n",
        "\n",
        "    mel_chunks = []\n",
        "    mel_idx_multiplier = 80./fps\n",
        "    i = 0\n",
        "    while 1:\n",
        "        start_idx = int(i * mel_idx_multiplier)\n",
        "        if start_idx + mel_step_size > len(mel[0]):\n",
        "            mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
        "            break\n",
        "        mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
        "        i += 1\n",
        "\n",
        "    print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
        "\n",
        "    full_frames = full_frames[:len(mel_chunks)]\n",
        "\n",
        "    batch_size = wav2lip_batch_size\n",
        "    gen = datagen(full_frames.copy(), mel_chunks,face_det_batch_size = face_det_batch_size,wav2lip_batch_size = wav2lip_batch_size)\n",
        "\n",
        "    for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen,\n",
        "                                            total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
        "        if i == 0:\n",
        "            frame_h, frame_w = full_frames[0].shape[:-1]\n",
        "            out = cv2.VideoWriter('temp/result.avi',\n",
        "                                    cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
        "\n",
        "        img_batch = torch.cuda.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2)))\n",
        "        mel_batch = torch.cuda.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2)))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(mel_batch, img_batch)\n",
        "\n",
        "        pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
        "\n",
        "        for p, f, c in zip(pred, frames, coords):\n",
        "            y1, y2, x1, x2 = c\n",
        "            p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
        "\n",
        "            f[y1:y2, x1:x2] = p\n",
        "            out.write(f)\n",
        "\n",
        "    out.release()\n",
        "    audio_source_quoted = shlex.quote(audio_source)\n",
        "    command = f'ffmpeg -y -i {audio_source_quoted} -i temp/result.avi -c:v h264_nvenc -preset fast -c:a aac -strict -2 -qp 18 -threads 4 results/{output_name}.mp4'\n",
        "    if not stream:\n",
        "\n",
        "\n",
        "      # Execute the FFmpeg command\n",
        "      try:\n",
        "          subprocess.run(command, shell=True, check=True)\n",
        "          print(\"FFmpeg operation completed successfully.\")\n",
        "      except subprocess.CalledProcessError as e:\n",
        "          print(f\"Error: {e}\")\n",
        "    else:\n",
        "      # FFmpeg command with multithreading, CUDA hardware acceleration, and qp option, streaming the output\n",
        "\n",
        "      # Execute the FFmpeg command and capture the output\n",
        "      try:\n",
        "          process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "          # Read and process the output stream (stdout and stderr) if needed\n",
        "          output, error = process.communicate()\n",
        "\n",
        "          # Your processing logic for the output stream goes here\n",
        "\n",
        "          print(\"FFmpeg operation completed successfully.\")\n",
        "      except subprocess.CalledProcessError as e:\n",
        "          print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "ptYqQLxohj2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to create upload a audio containing a question and run the function question2answer specifying the direction of the audio file, and the name of the AI generated voice."
      ],
      "metadata": {
        "id": "0O3OxvbhrL2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you need to upload the video/picture of the avatar that we want to create and paste it in the directory wav2lip/data then run the function run_inference (read the comments in the function for more information)"
      ],
      "metadata": {
        "id": "aEIg_vI0sNvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "example of how to create the avatar"
      ],
      "metadata": {
        "id": "rn1GTKKjtPjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let data/avatar_video.mp4 be the location where the video is located\n",
        "#let data/question.wav be the the location where the audio with the question is located\n",
        "question2answer(audio = data/question.wav,voice_name = 'Rachel')\n",
        "main(face = data/avatar_video.mp4)"
      ],
      "metadata": {
        "id": "zrrv6tNftThX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}