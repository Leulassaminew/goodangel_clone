{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "collapsed_sections": [
        "Ur9J8bUE50Oa",
        "A5bsQzvU6AbG",
        "WN1uldqU6LvB",
        "Lv8xSnK6X8V2",
        "h0kJya5A-xPL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PI6xG48jbFY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDFkbd6mbE_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG11QtCLAd5I",
        "outputId": "2564235a-e66c-4ef6-b362-1c4a30807bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAD-NeRF'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 124 (delta 21), reused 12 (delta 12), pack-reused 89\u001b[K\n",
            "Receiving objects: 100% (124/124), 111.88 KiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "/content/RAD-NeRF\n"
          ]
        }
      ],
      "source": [
        "#@title install dependencies\n",
        "!git clone https://github.com/ashawkey/RAD-NeRF.git\n",
        "\n",
        "%cd RAD-NeRF\n",
        "\n",
        "%mkdir -p pretrained\n",
        "%mkdir -p data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the model and Setting up Thin-Plate-Spline-Motion-Model for Generating Talking Avatar"
      ],
      "metadata": {
        "id": "CowET6UB5opM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1MB7jE9pl-k9aWd_TY3IEeCe9T3wfHOgT\n",
        "!unzip TPSMM.zip\n",
        "\n",
        "!gdown --id 1-CKOjv_y_TzNe-dwQsjjeVxJUuyBAb5X --output TPSMM/vox.pth.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt1F5OQh5c9c",
        "outputId": "b35c4f0b-17cb-4bc1-d5c0-183c583040ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MB7jE9pl-k9aWd_TY3IEeCe9T3wfHOgT\n",
            "To: /content/TPSMM.zip\n",
            "100% 67.1M/67.1M [00:00<00:00, 115MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgNo_Zwq5z21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing all the dependencies"
      ],
      "metadata": {
        "id": "Ur9J8bUE50Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install streamlit\n",
        "# !pip install -U elevenlabs\n",
        "# !pip install face_alignment\n",
        "# !pip install imageio_ffmpeg\n",
        "# !pip3 install wldhx.yadisk-direct\n",
        "# !pip install pyngrok openai\n",
        "# !pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "# !pip install gradio\n",
        "# !pip install streamlit-audiorecorder"
      ],
      "metadata": {
        "id": "xG6COn1GBJ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OikWLB4y55G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing all the Libraries and configuration for the RAD-NeRF model"
      ],
      "metadata": {
        "id": "A5bsQzvU6AbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install (slow...)\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "!bash scripts/install_ext.sh"
      ],
      "metadata": {
        "id": "5fEICu31AnRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bed8803-70fa-4aca-c058-0e7283a4cc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 2s (108 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Collecting torch-ema (from -r requirements.txt (line 1))\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting ninja (from -r requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trimesh (from -r requirements.txt (line 3))\n",
            "  Downloading trimesh-3.23.1-py3-none-any.whl (686 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m686.3/686.3 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.8.0.76)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 5))\n",
            "  Downloading tensorboardX-2.6.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.7.1)\n",
            "Collecting PyMCubes (from -r requirements.txt (line 11))\n",
            "  Downloading PyMCubes-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m274.3/274.3 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (13.5.2)\n",
            "Collecting dearpygui (from -r requirements.txt (line 13))\n",
            "  Downloading dearpygui-1.9.1-cp310-cp310-manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (23.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.10.1)\n",
            "Requirement already satisfied: face_alignment in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (1.4.0)\n",
            "Collecting python_speech_features (from -r requirements.txt (line 18))\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.56.4)\n",
            "Collecting resampy (from -r requirements.txt (line 20))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 21))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyaudio (from -r requirements.txt (line 22))\n",
            "  Downloading PyAudio-0.2.13.tar.gz (46 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (0.12.1)\n",
            "Collecting einops (from -r requirements.txt (line 24))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configargparse (from -r requirements.txt (line 25))\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting lpips (from -r requirements.txt (line 27))\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.4.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 8)) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (2.16.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from face_alignment->-r requirements.txt (line 17)) (0.19.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 19)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 19)) (67.7.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 21))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 21))\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r requirements.txt (line 23)) (1.15.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips->-r requirements.txt (line 27)) (0.15.2+cu118)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 23)) (2.21)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 21)) (2023.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 21)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 21)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 21)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 21)) (2023.7.22)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment->-r requirements.txt (line 17)) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment->-r requirements.txt (line 17)) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->face_alignment->-r requirements.txt (line 17)) (1.4.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Building wheels for collected packages: python_speech_features, pyaudio\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5870 sha256=cf2f133a5cc370b535cc7a6fbb9aa2e29736bb2d7a856645ae51610a7d11bc92\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.13-cp310-cp310-linux_x86_64.whl size=63863 sha256=6b2b7929de972d464cd3ec58e4f7018850e06aa6e683dbb62ce30c8d5cf52935\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/f1/c2/d102b4765a82c5a7bb273998dca7e4a53fc58e9a1a516fda81\n",
            "Successfully built python_speech_features pyaudio\n",
            "Installing collected packages: tokenizers, safetensors, python_speech_features, pyaudio, ninja, trimesh, tensorboardX, einops, dearpygui, configargparse, resampy, PyMCubes, transformers, torch-ema, lpips\n",
            "Successfully installed PyMCubes-0.1.4 configargparse-1.7 dearpygui-1.9.1 einops-0.6.1 lpips-0.1.4 ninja-1.11.1 pyaudio-0.2.13 python_speech_features-0.6 resampy-0.4.2 safetensors-0.3.2 tensorboardX-2.6.2 tokenizers-0.13.3 torch-ema-0.3 transformers-4.31.0 trimesh-3.23.1\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Processing ./freqencoder\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: freqencoder\n",
            "  Building wheel for freqencoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for freqencoder: filename=freqencoder-0.0.0-cp310-cp310-linux_x86_64.whl size=2488314 sha256=680762dda0cbbd3b74b56b3f2ce5e53383c0c3c4307f6fa157639eba830afaae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6xkgvn1n/wheels/e6/ae/7d/c0b71a6c1e5aff92aa16367c3432ceb60ad1cecbef617ec91a\n",
            "Successfully built freqencoder\n",
            "Installing collected packages: freqencoder\n",
            "Successfully installed freqencoder-0.0.0\n",
            "Processing ./shencoder\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: shencoder\n",
            "  Building wheel for shencoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shencoder: filename=shencoder-0.0.0-cp310-cp310-linux_x86_64.whl size=2562838 sha256=4515e848f3471bb3322856b914d9cf133f46fcb126a3cf469d2e6aaf248432ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t46yd_9j/wheels/47/ae/a1/8413cc37d82d2fdd6250006c1b1def72361587eaf0cbbfa1fe\n",
            "Successfully built shencoder\n",
            "Installing collected packages: shencoder\n",
            "Successfully installed shencoder-0.0.0\n",
            "Processing ./gridencoder\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gridencoder\n",
            "  Building wheel for gridencoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gridencoder: filename=gridencoder-0.0.0-cp310-cp310-linux_x86_64.whl size=5307531 sha256=91f78b0369c236371b05a8b7ed8b09ebed3141801ec2675c91d4301a95da66d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-canwu6nz/wheels/ff/21/28/1398b18efa04215165e77c67ad3615546bf27654ee3bdbdb51\n",
            "Successfully built gridencoder\n",
            "Installing collected packages: gridencoder\n",
            "Successfully installed gridencoder-0.0.0\n",
            "Processing ./raymarching\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: raymarching-face\n",
            "  Building wheel for raymarching-face (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for raymarching-face: filename=raymarching_face-0.0.0-cp310-cp310-linux_x86_64.whl size=2811845 sha256=9572bbb9a638c426c07cbc07f6c6c8f3eeda723901491585f9af926bb6dc231b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5f_gghx8/wheels/be/28/68/b1a633cbaaf5cd8bd6847f08a7f7d26f8078d55755b0d3acc7\n",
            "Successfully built raymarching-face\n",
            "Installing collected packages: raymarching-face\n",
            "Successfully installed raymarching-face-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeotTI9o6LRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start working on Model"
      ],
      "metadata": {
        "id": "WN1uldqU6LvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RAD-NeRF"
      ],
      "metadata": {
        "id": "AJ5EpK7WRpp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af81a082-7cdb-4353-b50e-a1ae1cb529b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RAD-NeRF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Person = 'engm' #@param ['obama', 'marco', 'engm', 'chris']\n",
        "Audio = 'custom' #@param ['intro', 'nvp', 'custom']\n",
        "Background = 'default' #@param ['default', 'custom']\n",
        "Pose_start = 0 #@param {type: 'integer'}\n",
        "Pose_end = 100 #@param {type: 'integer'}\n"
      ],
      "metadata": {
        "id": "bCc_LMuAaYYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvz18AiuXwxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download things for RAD-NeRF Repo\n",
        "\n",
        "import gdown\n",
        "\n",
        "# model checkpoint and pose sequence\n",
        "if Person == 'obama':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1dlajKAkibAWRzNWaUdoFO0OKhVeeBW9w\", \"pretrained/model.pth\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1_PK1AZCpB7CCBiQsvOm14qr31K20TV9J\", \"data/pose.json\", quiet=False)\n",
        "elif Person == 'engm':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1P4Fbg5IN_eWxkr7sVQJeK95WVnzV0yGs\", \"pretrained/model.pth\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1bmkohn1bnxkCJs9Igv5xXIBhyWKoyXMb\", \"data/pose.json\", quiet=False)\n",
        "elif Person == 'marco':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1XkLbWF3CbvIaLTkjUd7zkdp03um1cyPL\", \"pretrained/model.pth\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1ETV7yyK3u4MC1ubUQbIvTrRAfJvypcUg\", \"data/pose.json\", quiet=False)\n",
        "elif Person == 'chris':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1Mx2f1Iu-W9yG8QOn_2ROhhIcrVz4yMeA\", \"pretrained/model.pth\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1O8uvNuqMo4p0aJi16OMJXLsd1UeQ_7Dw\", \"data/pose.json\", quiet=False)\n",
        "\n",
        "\n",
        "# audio example\n",
        "if Audio == 'intro':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1gxfLuYuGFMlYz7BDxJlcfnDISN4sfbKg\", \"data/intro.wav\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1r2-wJ0VFfahE4bdDzGgvoC2tU24XmxPu\", \"data/intro_eo.npy\", quiet=False)\n",
        "elif Audio == 'nvp':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1WgtSiB_gLZZ-b8IO_KIVwT_ZRPUSx7yY\", \"data/nvp.wav\", quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1UKCW67Idw00_3QqzXasN_dvYXlYC37fe\", \"data/nvp_eo.npy\", quiet=False)\n",
        "\n",
        "\n",
        "# background image\n",
        "if Background == 'default':\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1RDiRCFXMk3hQ0hzKVculQdFqisayT1MM\", \"data/bg.jpg\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiDnJHYlAnbr",
        "outputId": "1ca67e82-c039-4c4f-d350-52f7e19258df",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P4Fbg5IN_eWxkr7sVQJeK95WVnzV0yGs\n",
            "To: /content/RAD-NeRF/pretrained/model.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.3M/17.3M [00:00<00:00, 31.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bmkohn1bnxkCJs9Igv5xXIBhyWKoyXMb\n",
            "To: /content/RAD-NeRF/data/pose.json\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.08M/2.08M [00:00<00:00, 237MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RDiRCFXMk3hQ0hzKVculQdFqisayT1MM\n",
            "To: /content/RAD-NeRF/data/bg.jpg\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.3k/51.3k [00:00<00:00, 48.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RAD-NeRF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vJ861juCJIm",
        "outputId": "5c685605-db46-45eb-ce22-9231a58af66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RAD-NeRF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Talking Avatar Template from RAD-NeRF"
      ],
      "metadata": {
        "id": "Lv8xSnK6X8V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#markdown ####**Settings:**\n",
        "\n",
        "import os, whisper\n",
        "import torchaudio\n",
        "import datetime\n",
        "import gradio as gr\n",
        "from subprocess import run\n",
        "from audio_avatar import generate_audio\n",
        "\n",
        "def avatar_generation(audio):\n",
        "    Person = 'engm' #param ['obama', 'marco', 'engm', 'chris']\n",
        "    Audio = 'custom' #param ['intro', 'nvp', 'custom']\n",
        "    Background = 'default' #param ['default', 'custom']\n",
        "    Pose_start = 0 #param {type: 'integer'}\n",
        "    Pose_end = 100 #param {type: 'integer'}\n",
        "\n",
        "    parent = 'data'\n",
        "    now = datetime.datetime.now()\n",
        "    time_ = now.time()\n",
        "    time_var = time_.isoformat().split('.')[0]\n",
        "    time_var = time_var.replace(':', '_')\n",
        "    audio_name = parent + '/' + time_var + '.wav'\n",
        "\n",
        "\n",
        "    audio_read, sr = torchaudio.load(audio)\n",
        "    torchaudio.save(audio_name, audio_read, sample_rate=sr)\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_name)\n",
        "    user_prompt = result[\"text\"]\n",
        "    chat_Aud = generate_audio(user_prompt)\n",
        "    # if Audio == 'custom':\n",
        "    print('Getting AI features...', '\\n')\n",
        "    run(['python', 'nerf/asr.py', '--wav', chat_Aud, '--save_feats'])\n",
        "\n",
        "    BG = 'bg.jpg'\n",
        "    audio_npy = 'data/' + chat_Aud.split('data/')[1][:-4] + '_eo.npy'\n",
        "    # audio_npy = '/content/RAD-NeRF/data/chatgpt_response_20_25_59_eo.npy'\n",
        "    bg_img = 'data/' + BG\n",
        "    print('Generating Avatar')\n",
        "\n",
        "    os.system(f\"python test.py -O --torso --pose data/pose.json --data_range {Pose_start} {Pose_end} --ckpt pretrained/model.pth --aud {audio_npy} --bg_img {bg_img} --workspace trail\")\n",
        "\n",
        "    Video = 'trail/results/ngp_ep0028.mp4'\n",
        "    Video_aud = Video.replace('.mp4', '_aud.mp4')\n",
        "\n",
        "    # concat audio\n",
        "    os.system(f\"ffmpeg -y -i {Video} -i {chat_Aud} -c:v copy -c:a aac {Video_aud}\")\n",
        "    return Video"
      ],
      "metadata": {
        "id": "0wMfVqIuAnUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3nVYxzEGXtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate talking avatar for the target image, with audio based generated video template"
      ],
      "metadata": {
        "id": "h0kJya5A-xPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, cv2\n",
        "import streamlit as st\n",
        "import time, os, shutil, datetime\n",
        "import imageio\n",
        "from moviepy.editor import *\n",
        "from PIL import Image\n",
        "import imageio_ffmpeg\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import warnings\n",
        "import os\n",
        "from TPSMM.demo import make_animation\n",
        "from skimage import img_as_ubyte\n",
        "# from rad_nerf import avatar_generation\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from TPSMM.demo import load_checkpoints\n",
        "from audio_avatar import generate_audio\n",
        "import streamlit as st\n",
        "import openai, whisper\n",
        "from elevenlabs import generate, play, save, set_api_key\n",
        "\n",
        "set_api_key('')\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "NFV7-2GyGXc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,  # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "\n",
        "def save_uploadedfile(uploadedfile):\n",
        "    os.makedirs('tempDir', exist_ok=True)\n",
        "    video_file_name = os.path.join(\"tempDir\", uploadedfile.name)\n",
        "    with open(video_file_name, \"wb\") as f:\n",
        "        f.write(uploadedfile.getbuffer())\n",
        "\n",
        "    return video_file_name\n",
        "\n",
        "\n",
        "def generate_animation(image_file, voice_file):\n",
        "    parent = 'data'\n",
        "    now = datetime.datetime.now()\n",
        "    time_ = now.time()\n",
        "    time_var = time_.isoformat().split('.')[0]\n",
        "    time_var = time_var.replace(':', '_')\n",
        "    audio_name = parent + '/' + time_var + '.wav'\n",
        "\n",
        "\n",
        "    audio_read, sr = torchaudio.load(voice_file)\n",
        "    torchaudio.save(audio_name, audio_read, sample_rate=sr)\n",
        "    # edit the config\n",
        "    device = torch.device('cuda:0')\n",
        "    dataset_name = 'vox'  # ['vox', 'taichi', 'ted', 'mgif']\n",
        "    # source_image_path = image\n",
        "    # driving_video_path = driving_video_1\n",
        "\n",
        "    # image_uploaded = save_uploadedfile(image_file)\n",
        "\n",
        "    try:\n",
        "        voice_file_name = save_uploadedfile(voice_file)\n",
        "    except:\n",
        "        voice_file_name = audio_name\n",
        "    print('Performing voice commands with ChatGPT response...')\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(voice_file_name)\n",
        "    voice_prompt = result[\"text\"]\n",
        "\n",
        "    print(\"\"\"Generating Voice ChatGPT Response...\"\"\")\n",
        "    chatgpt_response = get_completion(voice_prompt)\n",
        "    audio_response = generate_audio(chatgpt_response)\n",
        "\n",
        "    parent = '/content'\n",
        "    now = datetime.datetime.now()\n",
        "    time_ = now.time()\n",
        "    time_var = time_.isoformat().split('.')[0]\n",
        "    time_var = time_var.replace(':', '_')\n",
        "\n",
        "    output_video_path = parent + '/' + time_var + '_animation' + '.mp4'\n",
        "\n",
        "    config_path = 'TPSMM/config/vox-256.yaml'\n",
        "    checkpoint_path = 'TPSMM/vox.pth.tar'\n",
        "\n",
        "    predict_mode = 'relative'  # ['standard', 'relative', 'avd']\n",
        "    find_best_frame = False  # when use the relative mode to animate a face, use 'find_best_frame=True' can get better quality result\n",
        "\n",
        "    pixel = 256  # for vox, taichi and mgif, the resolution is 256*256\n",
        "    if (dataset_name == 'ted'):  # for ted, the resolution is 384*384\n",
        "        pixel = 384\n",
        "\n",
        "    # source_image = imageio.imread(image_uploaded)\n",
        "\n",
        "    print('Rendering video animation...', '\\n')\n",
        "    ### driving video rendering\n",
        "\n",
        "    os.makedirs('tempDir', exist_ok=True)\n",
        "\n",
        "    \"\"\"\n",
        "    Here we will generate talking avatar template based on our audio.\n",
        "    \"\"\"\n",
        "\n",
        "    talking_person = avatar_generation(audio_response)\n",
        "\n",
        "    reader = imageio.get_reader(talking_person)\n",
        "\n",
        "    source_image = resize(image_file, (pixel, pixel))[..., :3]\n",
        "\n",
        "    fps = reader.get_meta_data()['fps']\n",
        "    render_video = []\n",
        "    try:\n",
        "        for im in reader:\n",
        "            render_video.append(im)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "    reader.close()\n",
        "\n",
        "    driving_video = [resize(frame, (pixel, pixel))[..., :3] for frame in render_video]\n",
        "\n",
        "    inpainting, kp_detector, dense_motion_network, avd_network = load_checkpoints(config_path=config_path,\n",
        "                                                                                checkpoint_path=checkpoint_path,\n",
        "                                                                                device=device)\n",
        "\n",
        "    if predict_mode == 'relative' and find_best_frame:\n",
        "        from TPSMM.demo import find_best_frame as _find\n",
        "        i = _find(source_image, driving_video, device.type == 'cpu')\n",
        "        print(\"Best frame: \" + str(i))\n",
        "        driving_forward = driving_video[i:]\n",
        "        driving_backward = driving_video[:(i + 1)][::-1]\n",
        "        predictions_forward = make_animation(source_image, driving_forward, inpainting, kp_detector,\n",
        "                                            dense_motion_network, avd_network, device=device, mode=predict_mode)\n",
        "        predictions_backward = make_animation(source_image, driving_backward, inpainting, kp_detector,\n",
        "                                            dense_motion_network, avd_network, device=device, mode=predict_mode)\n",
        "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
        "    else:\n",
        "\n",
        "        print('Converting image to Digital Replica')\n",
        "        predictions = make_animation(source_image, driving_video, inpainting, kp_detector, dense_motion_network,\n",
        "                                    avd_network, device=device, mode=predict_mode)\n",
        "\n",
        "    # save resulting video\n",
        "    print(\"\"\"Saving animated video...\"\"\")\n",
        "    imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
        "\n",
        "    # audio_file = text_to_speech(chatgpt_response)\n",
        "\n",
        "    videoclip = VideoFileClip(output_video_path)\n",
        "    audioclip = AudioFileClip(audio_response)\n",
        "\n",
        "    new_audioclip = CompositeAudioClip([audioclip])\n",
        "    videoclip.audio = new_audioclip\n",
        "\n",
        "    audio_video_file = 'audio_video.mp4'\n",
        "    videoclip.write_videofile(audio_video_file)\n",
        "\n",
        "    return audio_video_file\n",
        "\n"
      ],
      "metadata": {
        "id": "KYofom5SFzJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    title = \"\"\"\n",
        "    ðŸ—£ï¸ðŸ”„ðŸ’¬ Communication with Cloneality Voice Cloner ðŸ”„ðŸ’¬ðŸ—£ï¸\"\"\",\n",
        "\n",
        "    description = \"\"\"\n",
        "    Welcome to Cloneality Voice Cloner, your gateway to next-level communication! ðŸš€ðŸŽ¤ Our innovative website combines the power of ChatGPT and ElevenLabs API, offering an unparalleled experience in voice-to-voice communication. ðŸ¤ðŸŽ™ï¸\n",
        "\n",
        "    Simply engage with ChatGPT, and witness its magic as it clones your voice, creating a lifelike audio representation. ðŸŽ‰ðŸ”Š Give voice instructions, share thoughts, and receive responses - all in your own voice! ðŸŽ¯ðŸ—£ï¸\n",
        "\n",
        "    With Voice-Cloner Connect, the possibilities are endless! Embrace the future of interactive and personalized conversations. ðŸŒðŸ”® Elevate your interactions today and unlock a new realm of communication! ðŸŒŸðŸ’¬\n",
        "\n",
        "    \"\"\",\n",
        "    fn=generate_animation,\n",
        "    inputs=[\n",
        "        gr.Image(label='Target Image'),\n",
        "        gr.Audio(source=\"microphone\", type=\"filepath\",)\n",
        "\n",
        "        #streaming=True)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Avatar Response\")\n",
        "    ]).launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "wC3KPha4gjlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Video\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def show_video(video_path, video_width=450):\n",
        "\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "show_video(Video_aud)\n"
      ],
      "metadata": {
        "id": "q55svflzAnnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MEvOFcHAoXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}